# Instrumentation Plan Critique & Recommendations

## Overall Assessment

The plan is **comprehensive and well-structured**, covering all five goal areas with thoughtful event definitions. However, there are several gaps, ambiguities, and areas for improvement identified below.

---

## Critical Gaps

### 1. **Missing: Run Lifecycle Tracking**

**Gap**: The plan tracks `feature_used` when a run is submitted, but doesn't track the full lifecycle:
- Run started
- Run in progress / progress updates
- Run completed successfully
- Run failed with error
- Run cancelled by user

**Why it matters**: From the goals: *"I want to know about people's use of tools: Can we make guesses about people's use of (e.g.) column-level diff, profile, etc?"*
- We need completion rates, not just initiation
- Failure analysis is critical for understanding friction
- Correlation between run type and success rate

**Recommendation**: Add `run_completed` event with:
```python
{
    'run_id': str,              # Hashed
    'run_type': str,
    'status': str,              # 'success', 'error', 'cancelled'
    'duration_seconds': float,
    'error_type': str | None,   # 'connection', 'timeout', 'query_error', etc.
    'from_context': str,        # Same as feature_used
    'had_cached_result': bool,  # Was this a cache hit?
}
```

### 2. **Missing: Error/Failure Tracking**

**Gap**: No explicit error event tracking beyond run failures.

**Examples of missing error scenarios**:
- Lineage load failures (manifest parsing errors)
- Database connection failures
- State sync failures (cloud mode)
- Share URL generation failures
- Check creation failures

**Why it matters**: Understanding where users hit walls is as important as tracking successful flows.

**Recommendation**: Add `error_encountered` event:
```python
{
    'error_category': str,      # 'lineage', 'database', 'state', 'share', 'check'
    'error_type': str,          # Specific error
    'context': str,             # What user was trying to do
    'recoverable': bool,        # Did we show recovery options?
}
```

### 3. **Missing: Session Boundaries**

**Gap**: No clear session start/end events.

**Why it matters**: To answer *"how long are typical sessions?"* and *"what percentage of users return?"*

**Recommendation**: Add:
- `session_started` - when server starts or frontend mounts
- `session_ended` - on server shutdown or tab close (if capturable)
- Include session_id in all events for sessionization

### 4. **Missing: "Empty State" Detection**

**Gap**: From goals: *"do we have a large number of users drawing only the lineage, without seeing the comparison?"*

The plan doesn't explicitly track:
- Users who view lineage but never run checks
- Users who create checks but never approve them
- Users who generate runs but never view results

**Recommendation**: Add derived metrics or periodic snapshot of:
```python
{
    'has_viewed_lineage': bool,
    'has_created_check': bool,
    'has_run_check': bool,
    'has_approved_check': bool,
    'has_viewed_results': bool,
    'time_to_first_check_minutes': float | None,
    'time_to_first_approval_minutes': float | None,
}
```

This could be included in `environment_snapshot` or as a separate `user_journey_milestone` event.

---

## Ambiguities & Clarifications Needed

### 5. **Ambiguous: `lineage_computed` Timing**

**Issue**: The event says "after DbtAdapter computes lineage diff" but the implementation note says "fire once per lineage load, not on every query."

**Questions**:
- Does this fire on server startup?
- Does it fire when artifacts are reloaded (file watch)?
- What if lineage is computed but errors?

**Recommendation**: Be explicit:
- Fire on successful lineage computation
- Include `trigger` property: 'server_startup', 'file_watch', 'manual_refresh'
- Include `computation_time_seconds`

### 6. **Ambiguous: Frontend Event Transport**

**Issue**: Plan says *"For frontend events, call backend API endpoints that trigger tracking"*

**Questions**:
- Are we adding new API endpoints just for tracking?
- Or using existing Amplitude SDK in frontend (which already exists)?

**Recommendation**: **Use frontend Amplitude SDK directly for UI events** (already in place via `track.ts`). Only use backend for:
- Events that need backend context (database info, state metadata)
- Events triggered by backend operations

This reduces latency and backend load.

### 7. **Ambiguous: `access_method` Detection**

In `shared_state_accessed`, how do we detect `'direct_link', 'pr_comment', 'slack', 'unknown'`?

**Recommendation**: Use HTTP Referer header as best effort, but accept that most will be 'unknown'. Consider:
- Add query param `?source=slack` to share URLs generated by Slack unfurl
- Add `?source=pr_comment` to URLs in PR comments

### 8. **Ambiguous: Multi-User Session Detection**

*"Backend - periodic check in cloud state loader"*

**Questions**:
- How do we know multiple users are on same state?
- What defines "concurrent" vs "sequential"?
- How often do we check?

**Recommendation**:
- Track when state is accessed (via share URL or PR)
- Fire event when 2+ distinct user_ids access same state within N minutes
- Consider cloud provider access logs if available

---

## Missing Context Properties

### 9. **Dataset Context Missing from User Interaction Events**

**Issue**: Events like `tab_changed`, `model_selected`, `query_builder_used` don't include dataset context.

**Why it matters**: From goals: *"Can we correlate these with hypotheses about changed data, new columns, etc?"*

**Recommendation**: Add to user interaction events:
```python
{
    # ... existing properties
    'dataset_size': str,        # 'small' (<10 nodes), 'medium' (10-100), 'large' (>100)
    'num_changes': int,         # Number of changed nodes in current session
    'has_schema_changes': bool, # Are there column-level changes?
}
```

This allows correlation analysis between dataset characteristics and user behavior.

### 10. **Missing: Query Complexity Metrics**

**Issue**: `query_builder_used` tracks if query exists, but not complexity.

**Why it matters**: Understanding if users write simple vs complex queries.

**Recommendation**: Add (while respecting privacy - no actual SQL):
```python
{
    # ... existing properties
    'query_length_chars': int,      # Rough proxy for complexity
    'has_joins': bool,
    'has_aggregations': bool,
    'has_where_clause': bool,
}
```

Parse SQL safely to extract structural features, not values.

---

## Implementation Concerns

### 11. **Performance: `environment_snapshot` Frequency**

**Issue**: Proposed 15-minute interval may be too frequent for long-running servers.

**Concerns**:
- Event volume
- State computation overhead
- Redundant data (most properties don't change mid-session)

**Recommendation**:
- Fire once at server startup (captures 90% of value)
- Fire on state changes (PR merged, artifacts reloaded)
- **Skip periodic snapshots** unless we see evidence of long-lived sessions with state changes

### 12. **Privacy: Node IDs Should Be Hashed Consistently**

**Issue**: Plan says "Hashed node ID" but doesn't specify how.

**Recommendation**:
- Use SHA256 like existing repo/branch hashing
- Apply consistently across all events
- Document in privacy section: "Node IDs are hashed per-user-session, not globally" (if using session salt)

### 13. **Implementation: `has_database` Detection**

*"Test database connection at startup (try connecting via adapter)"*

**Concern**: This could slow server startup significantly.

**Recommendation**:
- Don't actually connect - check if adapter is initialized with credentials
- Or use lazy detection - fire `database_connected` event on first successful query
- Add `database_connection_tested` flag to indicate if we've verified connectivity

---

## Missing from Original Goals

### 14. **"Virtually Every Button" Instrumentation**

From goals: *"virtually every button should come with some degree of instrumentation"*

**Missing buttons/actions**:
- Refresh lineage button
- Filter toggles (show only changed, show only models, etc.)
- Sort controls
- Search/find in lineage
- Expand/collapse all nodes
- Download/export actions (CSV, JSON, etc.)
- Settings/preferences changes
- Help/documentation clicks

**Recommendation**: Add generic `ui_action` event:
```python
{
    'action_type': str,         # 'button_click', 'toggle', 'filter', 'sort'
    'action_name': str,         # 'refresh_lineage', 'filter_changed_only', etc.
    'location': str,            # 'lineage_view', 'checks_panel', 'settings'
    'action_value': str | None, # For filters/toggles, the new value
}
```

### 15. **Missing: Correlation with Session Replay**

From goals: *"Use qualitative analysis to watch users working with their data (thanks, replay recording)"*

**Issue**: No mention of how to correlate Amplitude events with session replay (Sentry, LogRocket, etc.)

**Recommendation**: Ensure all events include:
```python
{
    'session_id': str,          # For correlation
    'replay_id': str | None,    # If session replay is active
}
```

### 16. **Missing: A/B Test Support**

**Gap**: No infrastructure for tracking experiment variants.

**Recommendation**: Add to all events:
```python
{
    'experiments': dict,        # {'feature_x': 'variant_a', 'ui_test_y': 'control'}
}
```

This future-proofs for experimentation.

---

## Event Naming Inconsistencies

### 17. **Inconsistent Naming Conventions**

**Issues**:
- Existing: `[Web] multi_nodes_action` vs proposed `model_selected`
- Existing: `share_state` vs proposed `share_initiated`
- Existing: `Column level lineage` (with space) vs proposed snake_case

**Recommendation**:
- Standardize on snake_case without prefixes for new events
- Or adopt `[Web]` prefix consistently for all frontend events
- Update existing events for consistency (migration plan needed)

---

## Opportunities for Enrichment

### 18. **Add "Outcome" Properties**

**Enhancement**: Track not just actions, but outcomes.

**Examples**:
- `check_created` → add `num_existing_checks` (is this their first?)
- `tab_changed` → add `num_items_in_tab` (are they switching to empty tab?)
- `share_initiated` → add `share_success: bool`

### 19. **Add "Path" Context**

**Enhancement**: Track how user got to current state.

**Example**:
```python
{
    'previous_action': str,     # Last event fired
    'actions_in_session': int,  # Event count this session
    'time_since_session_start_seconds': float,
}
```

This enables funnel and path analysis.

### 20. **Add Feature Flags to Context**

**Enhancement**: If you have feature flags (LaunchDarkly, etc.), include in all events:

```python
{
    'feature_flags': dict,      # Active flags for this user
}
```

---

## Documentation Gaps

### 21. **Missing: Event Schema Versioning**

**Issue**: No plan for evolving event schemas over time.

**Recommendation**:
- Add `schema_version: int` to all events
- Document migration strategy when properties change
- Consider using Amplitude's event schema validation

### 22. **Missing: Dashboard/Query Examples**

**Gap**: Plan doesn't show example Amplitude queries to answer the original questions.

**Recommendation**: Add section showing:
- "X% of users have cloud mode" → Query: `SELECT cloud_mode, COUNT(*) FROM environment_snapshot GROUP BY cloud_mode`
- "Median changed models" → Query: `SELECT PERCENTILE(changed_nodes, 0.5) FROM lineage_computed`

This validates that events capture the right data.

---

## Prioritization Questions

### 23. **Phase Priorities May Be Misaligned**

**Current plan**: Backend infrastructure → Dataset → Frontend → Collaboration → Testing

**Alternative**: Start with **highest-value questions first**:
1. **Configuration tracking** (Week 1) - answers "are users stuck?"
2. **User interaction tracking** (Week 1-2) - answers "how do people use the app?"
3. **Checklist operations** (Week 2) - answers "are checklists working?"
4. **Dataset tracking** (Week 3) - answers "how big are datasets?"
5. **Collaboration** (Week 4) - answers collaboration questions (possibly lower volume)

**Recommendation**: Re-sequence phases based on question priority, not technical convenience.

---

## Summary of Recommendations

### Must-Add (Critical)
1. ✅ Run completion/failure tracking
2. ✅ Error event tracking
3. ✅ Session boundary events
4. ✅ Empty state / user journey milestones
5. ✅ Clarify frontend vs backend event routing

### Should-Add (High Value)
6. ✅ Dataset context in UI events (for correlation)
7. ✅ Generic `ui_action` for "every button"
8. ✅ Consistent event naming convention
9. ✅ Query complexity metrics (privacy-safe)
10. ✅ Outcome properties (success/failure)

### Nice-to-Have (Future-Proofing)
11. ✅ A/B test support
12. ✅ Feature flag context
13. ✅ Path/funnel context
14. ✅ Event schema versioning
15. ✅ Session replay correlation

### Clarifications Needed
16. ❓ How to detect `access_method` for shared states?
17. ❓ How to implement multi-user session detection?
18. ❓ Should we test database connection at startup?
19. ❓ What triggers `environment_snapshot` exactly?

---

## Questions for You

1. **Priority**: Which original questions are highest priority to answer first?
2. **Session definition**: How do you want to define a "session"? (Server uptime? Tab lifetime? Time-based?)
3. **Collaboration tracking**: Do you have cloud infrastructure logs we can use, or rely on application-level tracking only?
4. **Experiment framework**: Do you have/plan feature flagging? Should we build that in now?
5. **Session replay**: Are you using Sentry SR, LogRocket, or other? Need correlation IDs?
6. **Timeline**: Is 4-week timeline realistic, or should we phase differently?

---

## Positive Aspects (Don't Change)

- ✅ Clear mapping to original goals
- ✅ Privacy considerations well thought out
- ✅ Good balance of backend vs frontend events
- ✅ Specific property definitions (not vague)
- ✅ Implementation notes for each event
- ✅ Recognition of existing infrastructure
